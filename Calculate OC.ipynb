{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##TÍNH OC"
      ],
      "metadata": {
        "id": "CupDCShSSnMN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqU1tedSSlDE"
      },
      "outputs": [],
      "source": [
        "!pip install pyvi\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 âm tiết\n",
        "import pandas as pd\n",
        "from pyvi import ViTokenizer\n",
        "\n",
        "sentiment_dict = pd.read_excel(\"/content/2_âm_tiết.xlsx\")\n",
        "\n",
        "def load_sentiment_dictionary(sentiment_dict):\n",
        "    overconfident_keywords = set()\n",
        "    nonconfident_keywords = set()\n",
        "    cautious_keywords = set()\n",
        "\n",
        "    for _, row in sentiment_dict.iterrows():\n",
        "        overconfident_keyword = row[\"overconfident\"]\n",
        "        nonconfident_keyword = row[\"nonconfident\"]\n",
        "        cautious_keyword = row[\"cautious\"]\n",
        "\n",
        "        if pd.notna(overconfident_keyword):\n",
        "            overconfident_keywords.add(str(overconfident_keyword).lower())\n",
        "\n",
        "        if pd.notna(nonconfident_keyword):\n",
        "            nonconfident_keywords.add(str(nonconfident_keyword).lower())\n",
        "\n",
        "        if pd.notna(cautious_keyword):\n",
        "            cautious_keywords.add(str(cautious_keyword).lower())\n",
        "\n",
        "    return overconfident_keywords, nonconfident_keywords, cautious_keywords\n",
        "\n",
        "def analyze_sentiment_vietnamese(article, overconfident_keywords, nonconfident_keywords, cautious_keywords):\n",
        "    article_str = str(article).lower()\n",
        "    tokens = ViTokenizer.tokenize(article_str).split()\n",
        "\n",
        "    overconfident_score = 0\n",
        "    nonconfident_score = 0\n",
        "    cautious_score = 0\n",
        "    for i in range(len(tokens) - 1):\n",
        "        word_pair = ' '.join([tokens[i], tokens[i+1]])\n",
        "        if word_pair in overconfident_keywords:\n",
        "            overconfident_score += 1\n",
        "        elif word_pair in nonconfident_keywords:\n",
        "            nonconfident_score += 1\n",
        "        elif word_pair in cautious_keywords:\n",
        "            cautious_score += 1\n",
        "\n",
        "\n",
        "    if overconfident_score > nonconfident_score and overconfident_score > cautious_score:\n",
        "        return \"Overconfident\"\n",
        "    elif nonconfident_score > overconfident_score and nonconfident_score > cautious_score:\n",
        "        return \"Nonconfident\"\n",
        "    elif cautious_score > overconfident_score and cautious_score > nonconfident_score:\n",
        "        return \"Cautious\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "\n",
        "news_file_path = \"/content/Pham_Nhat_Vuong.xlsx\"\n",
        "news_dataframe = pd.read_excel(news_file_path)\n",
        "\n",
        "overconfident_keywords, nonconfident_keywords, cautious_keywords = load_sentiment_dictionary(sentiment_dict)\n",
        "\n",
        "results_dataframe_2 = pd.DataFrame(columns=[\"Published year\", \"Keywords\", \"Title\", \"Sentiment\", \"Article Text\"])\n",
        "\n",
        "for index, row in news_dataframe.iterrows():\n",
        "    published_year = row[\"Published Year\"]\n",
        "    keywords = row[\"Keyword\"]\n",
        "    title = row[\"Title\"]\n",
        "    article = row[\"Article Text\"]\n",
        "\n",
        "    if pd.notna(article):\n",
        "        sentiment = analyze_sentiment_vietnamese(article, overconfident_keywords, nonconfident_keywords, cautious_keywords)\n",
        "\n",
        "        results_dataframe_2.loc[index] = [published_year, keywords, title, sentiment, article]\n",
        "\n",
        "print(results_dataframe_2)"
      ],
      "metadata": {
        "id": "MFloAxKBSySi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3 âm tiết\n",
        "sentiment_dict = pd.read_excel(\"/content/3_âm_tiết.xlsx\")\n",
        "\n",
        "def load_sentiment_dictionary(sentiment_dict):\n",
        "    overconfident_keywords = set()\n",
        "    nonconfident_keywords = set()\n",
        "    cautious_keywords = set()\n",
        "\n",
        "    for _, row in sentiment_dict.iterrows():\n",
        "        overconfident_keyword = row[\"overconfident\"]\n",
        "        nonconfident_keyword = row[\"nonconfident\"]\n",
        "        cautious_keyword = row[\"cautious\"]\n",
        "\n",
        "        if pd.notna(overconfident_keyword):\n",
        "            overconfident_keywords.add(str(overconfident_keyword).lower())\n",
        "\n",
        "        if pd.notna(nonconfident_keyword):\n",
        "            nonconfident_keywords.add(str(nonconfident_keyword).lower())\n",
        "\n",
        "        if pd.notna(cautious_keyword):\n",
        "            cautious_keywords.add(str(cautious_keyword).lower())\n",
        "\n",
        "    return overconfident_keywords, nonconfident_keywords, cautious_keywords\n",
        "\n",
        "def analyze_sentiment_vietnamese(article, overconfident_keywords, nonconfident_keywords, cautious_keywords):\n",
        "    article_str = str(article).lower()\n",
        "    tokens = ViTokenizer.tokenize(article_str).split()\n",
        "\n",
        "    overconfident_score = 0\n",
        "    nonconfident_score = 0\n",
        "    cautious_score = 0\n",
        "    for i in range(len(tokens) - 2):\n",
        "        word_pair = ' '.join([tokens[i], tokens[i+1], tokens[i+2]])\n",
        "        if word_pair in overconfident_keywords:\n",
        "            overconfident_score += 1\n",
        "        elif word_pair in nonconfident_keywords:\n",
        "            nonconfident_score += 1\n",
        "        elif word_pair in cautious_keywords:\n",
        "            cautious_score += 1\n",
        "\n",
        "\n",
        "    if overconfident_score > nonconfident_score and overconfident_score > cautious_score:\n",
        "        return \"Overconfident\"\n",
        "    elif nonconfident_score > overconfident_score and nonconfident_score > cautious_score:\n",
        "        return \"Nonconfident\"\n",
        "    elif cautious_score > overconfident_score and cautious_score > nonconfident_score:\n",
        "        return \"Cautious\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "\n",
        "news_file_path = \"/content/Pham_Nhat_Vuong.xlsx\"\n",
        "news_dataframe = pd.read_excel(news_file_path)\n",
        "\n",
        "overconfident_keywords, nonconfident_keywords, cautious_keywords = load_sentiment_dictionary(sentiment_dict)\n",
        "\n",
        "results_dataframe_3 = pd.DataFrame(columns=[\"Published year\", \"Keywords\", \"Title\", \"Sentiment\", \"Article Text\"])\n",
        "\n",
        "for index, row in news_dataframe.iterrows():\n",
        "    published_year = row[\"Published Year\"]\n",
        "    keywords = row[\"Keyword\"]\n",
        "    title = row[\"Title\"]\n",
        "    article = row[\"Article Text\"]\n",
        "\n",
        "    if pd.notna(article):\n",
        "        sentiment = analyze_sentiment_vietnamese(article, overconfident_keywords, nonconfident_keywords, cautious_keywords)\n",
        "\n",
        "        results_dataframe_3.loc[index] = [published_year, keywords, title, sentiment, article]\n",
        "\n",
        "print(results_dataframe_3)"
      ],
      "metadata": {
        "id": "pK6Hoa-xS1-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4 âm tiết\n",
        "\n",
        "sentiment_dict = pd.read_excel(\"/content/4_âm_tiết.xlsx\")\n",
        "\n",
        "def load_sentiment_dictionary(sentiment_dict):\n",
        "    overconfident_keywords = set()\n",
        "    nonconfident_keywords = set()\n",
        "    cautious_keywords = set()\n",
        "\n",
        "    for _, row in sentiment_dict.iterrows():\n",
        "        overconfident_keyword = row[\"overconfident\"]\n",
        "        nonconfident_keyword = row[\"nonconfident\"]\n",
        "        cautious_keyword = row[\"cautious\"]\n",
        "\n",
        "        if pd.notna(overconfident_keyword):\n",
        "            overconfident_keywords.add(str(overconfident_keyword).lower())\n",
        "\n",
        "        if pd.notna(nonconfident_keyword):\n",
        "            nonconfident_keywords.add(str(nonconfident_keyword).lower())\n",
        "\n",
        "        if pd.notna(cautious_keyword):\n",
        "            cautious_keywords.add(str(cautious_keyword).lower())\n",
        "\n",
        "    return overconfident_keywords, nonconfident_keywords, cautious_keywords\n",
        "\n",
        "def analyze_sentiment_vietnamese(article, overconfident_keywords, nonconfident_keywords, cautious_keywords):\n",
        "    article_str = str(article).lower()\n",
        "    tokens = ViTokenizer.tokenize(article_str).split()\n",
        "\n",
        "    overconfident_score = 0\n",
        "    nonconfident_score = 0\n",
        "    cautious_score = 0\n",
        "    for i in range(len(tokens) - 3):\n",
        "        word_pair = ' '.join([tokens[i], tokens[i+1], tokens[i+2], tokens[i+3]])\n",
        "        if word_pair in overconfident_keywords:\n",
        "            overconfident_score += 1\n",
        "        elif word_pair in nonconfident_keywords:\n",
        "            nonconfident_score += 1\n",
        "        elif word_pair in cautious_keywords:\n",
        "            cautious_score += 1\n",
        "\n",
        "\n",
        "    if overconfident_score > nonconfident_score and overconfident_score > cautious_score:\n",
        "        return \"Overconfident\"\n",
        "    elif nonconfident_score > overconfident_score and nonconfident_score > cautious_score:\n",
        "        return \"Nonconfident\"\n",
        "    elif cautious_score > overconfident_score and cautious_score > nonconfident_score:\n",
        "        return \"Cautious\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "\n",
        "news_file_path = \"/content/Pham_Nhat_Vuong.xlsx\"\n",
        "news_dataframe = pd.read_excel(news_file_path)\n",
        "\n",
        "overconfident_keywords, nonconfident_keywords, cautious_keywords = load_sentiment_dictionary(sentiment_dict)\n",
        "\n",
        "results_dataframe_4 = pd.DataFrame(columns=[\"Published year\", \"Keywords\", \"Title\", \"Sentiment\", \"Article Text\"])\n",
        "\n",
        "for index, row in news_dataframe.iterrows():\n",
        "    published_year = row[\"Published Year\"]\n",
        "    keywords = row[\"Keyword\"]\n",
        "    title = row[\"Title\"]\n",
        "    article = row[\"Article Text\"]\n",
        "\n",
        "    if pd.notna(article):\n",
        "        sentiment = analyze_sentiment_vietnamese(article, overconfident_keywords, nonconfident_keywords, cautious_keywords)\n",
        "\n",
        "        results_dataframe_4.loc[index] = [published_year, keywords, title, sentiment, article]\n",
        "\n",
        "print(results_dataframe_4)"
      ],
      "metadata": {
        "id": "zmFDIW18S24b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5 âm tiết\n",
        "\n",
        "sentiment_dict = pd.read_excel(\"/content/5_âm_tiết.xlsx\")\n",
        "\n",
        "def load_sentiment_dictionary(sentiment_dict):\n",
        "    overconfident_keywords = set()\n",
        "    nonconfident_keywords = set()\n",
        "    cautious_keywords = set()\n",
        "\n",
        "    for _, row in sentiment_dict.iterrows():\n",
        "        overconfident_keyword = row[\"overconfident\"]\n",
        "        nonconfident_keyword = row[\"nonconfident\"]\n",
        "        cautious_keyword = row[\"cautious\"]\n",
        "\n",
        "        if pd.notna(overconfident_keyword):\n",
        "            overconfident_keywords.add(str(overconfident_keyword).lower())\n",
        "\n",
        "        if pd.notna(nonconfident_keyword):\n",
        "            nonconfident_keywords.add(str(nonconfident_keyword).lower())\n",
        "\n",
        "        if pd.notna(cautious_keyword):\n",
        "            cautious_keywords.add(str(cautious_keyword).lower())\n",
        "\n",
        "    return overconfident_keywords, nonconfident_keywords, cautious_keywords\n",
        "\n",
        "def analyze_sentiment_vietnamese(article, overconfident_keywords, nonconfident_keywords, cautious_keywords):\n",
        "    article_str = str(article).lower()\n",
        "    tokens = ViTokenizer.tokenize(article_str).split()\n",
        "\n",
        "    overconfident_score = 0\n",
        "    nonconfident_score = 0\n",
        "    cautious_score = 0\n",
        "    for i in range(len(tokens) - 4):\n",
        "        word_pair = ' '.join([tokens[i], tokens[i+1], tokens[i+2], tokens[i+3], tokens[i+4]])\n",
        "        if word_pair in overconfident_keywords:\n",
        "            overconfident_score += 1\n",
        "        elif word_pair in nonconfident_keywords:\n",
        "            nonconfident_score += 1\n",
        "        elif word_pair in cautious_keywords:\n",
        "            cautious_score += 1\n",
        "\n",
        "\n",
        "    if overconfident_score > nonconfident_score and overconfident_score > cautious_score:\n",
        "        return \"Overconfident\"\n",
        "    elif nonconfident_score > overconfident_score and nonconfident_score > cautious_score:\n",
        "        return \"Nonconfident\"\n",
        "    elif cautious_score > overconfident_score and cautious_score > nonconfident_score:\n",
        "        return \"Cautious\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "\n",
        "news_file_path = \"/content/Pham_Nhat_Vuong.xlsx\"\n",
        "news_dataframe = pd.read_excel(news_file_path)\n",
        "\n",
        "overconfident_keywords, nonconfident_keywords, cautious_keywords = load_sentiment_dictionary(sentiment_dict)\n",
        "\n",
        "results_dataframe_5 = pd.DataFrame(columns=[\"Published year\", \"Keywords\", \"Title\", \"Sentiment\", \"Article Text\"])\n",
        "\n",
        "for index, row in news_dataframe.iterrows():\n",
        "    published_year = row[\"Published Year\"]\n",
        "    keywords = row[\"Keyword\"]\n",
        "    title = row[\"Title\"]\n",
        "    article = row[\"Article Text\"]\n",
        "\n",
        "    if pd.notna(article):\n",
        "        sentiment = analyze_sentiment_vietnamese(article, overconfident_keywords, nonconfident_keywords, cautious_keywords)\n",
        "\n",
        "        results_dataframe_5.loc[index] = [published_year, keywords, title, sentiment, article]\n",
        "\n",
        "print(results_dataframe_5)"
      ],
      "metadata": {
        "id": "rJguJW6zS5rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "excel_file = 'Pham_Nhat_Vuong_data_2.xlsx'\n",
        "results_dataframe_2.to_excel(excel_file, index=False)\n",
        "print(f\"Data has been combined and saved to {excel_file}.\")\n",
        "\n",
        "excel_file = 'Pham_Nhat_Vuong_data_3.xlsx'\n",
        "results_dataframe_3.to_excel(excel_file, index=False)\n",
        "print(f\"Data has been combined and saved to {excel_file}.\")\n",
        "\n",
        "excel_file = 'Pham_Nhat_Vuong_data_4.xlsx'\n",
        "results_dataframe_4.to_excel(excel_file, index=False)\n",
        "print(f\"Data has been combined and saved to {excel_file}.\")\n",
        "\n",
        "excel_file = 'Pham_Nhat_Vuong_data_5.xlsx'\n",
        "results_dataframe_5.to_excel(excel_file, index=False)\n",
        "print(f\"Data has been combined and saved to {excel_file}.\")"
      ],
      "metadata": {
        "id": "R1Zk03UzS7zB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "results_dataframe = pd.read_excel('/content/Pham_Nhat_Vuong_data_2.xlsx')"
      ],
      "metadata": {
        "id": "9N0zqukRTAD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_counts = results_dataframe.groupby(['Keywords', 'Published year'])['Sentiment'].value_counts().unstack()\n",
        "\n",
        "sentiment_counts['Overconfident'] = sentiment_counts['Overconfident'].fillna(0)\n",
        "\n",
        "if 'Cautious' not in sentiment_counts.columns:\n",
        "    sentiment_counts['Cautious'] = 0\n",
        "else:\n",
        "    sentiment_counts['Cautious'] = sentiment_counts['Cautious'].fillna(0)\n",
        "\n",
        "if 'Nonconfident' not in sentiment_counts.columns:\n",
        "    sentiment_counts['Nonconfident'] = 0\n",
        "else:\n",
        "    sentiment_counts['Nonconfident'] = sentiment_counts['Nonconfident'].fillna(0)\n",
        "\n",
        "\n",
        "sentiment_counts['Press Coverage'] = (sentiment_counts['Overconfident'] - (sentiment_counts['Cautious'] - sentiment_counts['Nonconfident'])) / sentiment_counts.sum(axis=1)\n",
        "\n",
        "output_dataframe = sentiment_counts.reset_index()[['Keywords', 'Published year', 'Press Coverage']]\n",
        "\n",
        "print(sentiment_counts['Press Coverage'])"
      ],
      "metadata": {
        "id": "eqYsEAIgTAz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_file = 'Pham_Nhat_Vuong_output.xlsx'\n",
        "output_dataframe.to_excel(Data_file, index=False)\n",
        "print(f\"Data has been combined and saved to {Data_file}.\")"
      ],
      "metadata": {
        "id": "U5xuvPDFTNbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##VẼ MA TRẬN TƯƠNG QUAN"
      ],
      "metadata": {
        "id": "nYStPctVTwvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "\n",
        "import seaborn as sns\n",
        "import statistics\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.stats import skew, kurtosis\n",
        "from sklearn import *\n",
        "import math"
      ],
      "metadata": {
        "id": "mOIS0pTzTtoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/Final_Data_2.xls'  # Corrected file name\n",
        "df = pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "8M7wg6pcT7yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify column names\n",
        "print(df.columns)\n",
        "\n",
        "# Check for leading/trailing whitespaces\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Verify column names again\n",
        "print(df.columns)\n",
        "\n",
        "# Create df_1 with desired columns\n",
        "df_1 = df[['ID', 'ROA',  'OC_CEO','CEOownership*OC','Firm_Size','Cash Dividends', 'D/E', 'Investment', 'ROA - 1']].copy()\n",
        "df_2 = df[['ID', 'ROA',  'OC_CEO','CEOownership','Firm_Size','Cash Dividends', 'D/E', 'Investment', 'ROA - 1']].copy()\n",
        "# Verify df_1\n",
        "print(df_1.head())"
      ],
      "metadata": {
        "id": "yZIjiA-2T9t2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only numeric columns for correlation calculation\n",
        "numeric_columns = df_1.select_dtypes(include=[int, float]).columns\n",
        "numeric_df = df_1[numeric_columns]\n",
        "\n",
        "# Compute the correlation matrix\n",
        "corr_matrix = numeric_df.corr()\n",
        "\n",
        "# Create a heatmap of the correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', square=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3uWB7gpvUC4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only numeric columns for correlation calculation\n",
        "numeric_columns = df_2.select_dtypes(include=[int, float]).columns\n",
        "numeric_df = df_2[numeric_columns]\n",
        "\n",
        "# Compute the correlation matrix\n",
        "corr_matrix = numeric_df.corr()\n",
        "\n",
        "# Create a heatmap of the correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', square=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MAa6UGyeUFRJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}